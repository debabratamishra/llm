{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2c5953",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE) vs Feed-Forward Networks (FFN) Comparison\n",
    "\n",
    "This notebook demonstrates a comprehensive comparison between Mixture of Experts (MoE) models and standard Feed Forward Networks (FFN). We'll explore their performance on multi modal tasks, parameter efficiency, and expert specialization patterns.\n",
    "\n",
    "## Key Learning Objectives:\n",
    "- Understanding MoE architecture and routing mechanisms\n",
    "- Comparing parameter efficiency between MoE and FFN models\n",
    "- Analyzing expert specialization on different task modalities\n",
    "- Visualizing training dynamics and convergence patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5488e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from typing import Tuple, Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "# Set clean plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ed909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset(Dataset):\n",
    "    \"\"\"Multi-modal dataset designed to showcase MoE advantages.\n",
    "    Four distinct modalities that benefit from specialized expert processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples: int = 10000, seq_len: int = 32, noise: float = 0.08):\n",
    "        super().__init__()\n",
    "        self.n = n_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.noise = noise\n",
    "        \n",
    "        samples_per_modality = n_samples // 4\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        # Modality 0: High-frequency sinusoidal patterns\n",
    "        for _ in range(samples_per_modality):\n",
    "            freq = np.random.uniform(1.0, 4.0)\n",
    "            phase = np.random.uniform(0, 2*np.pi)\n",
    "            amp = np.random.uniform(0.5, 1.5)\n",
    "            t = np.linspace(0, 6*np.pi, seq_len)\n",
    "            signal = amp * np.sin(freq * t + phase) + np.random.randn(seq_len) * noise\n",
    "            X.append(signal)\n",
    "            y.append(0)\n",
    "        \n",
    "        # Modality 1: Complex polynomial patterns\n",
    "        for _ in range(samples_per_modality):\n",
    "            # Higher-order polynomials\n",
    "            coeffs = np.random.randn(4) * [0.1, 0.3, 0.5, 0.2]  # cubic\n",
    "            t = np.linspace(-2, 2, seq_len)\n",
    "            signal = np.polyval(coeffs, t) + np.random.randn(seq_len) * noise\n",
    "            X.append(signal)\n",
    "            y.append(1)\n",
    "        \n",
    "        # Modality 2: Multi-level step functions\n",
    "        for _ in range(samples_per_modality):\n",
    "            n_steps = np.random.randint(3, 7)\n",
    "            signal = np.zeros(seq_len)\n",
    "            step_size = seq_len // n_steps\n",
    "            for i in range(n_steps):\n",
    "                start = i * step_size\n",
    "                end = min((i+1) * step_size, seq_len)\n",
    "                level = np.random.uniform(-1.5, 1.5)\n",
    "                signal[start:end] = level\n",
    "            signal += np.random.randn(seq_len) * noise * 0.3\n",
    "            X.append(signal)\n",
    "            y.append(2)\n",
    "        \n",
    "        # Modality 3: Exponential decay/growth patterns\n",
    "        for _ in range(samples_per_modality):\n",
    "            decay_rate = np.random.uniform(-0.15, 0.15)\n",
    "            initial = np.random.uniform(-1, 1)\n",
    "            t = np.linspace(0, seq_len-1, seq_len)\n",
    "            signal = initial * np.exp(decay_rate * t) + np.random.randn(seq_len) * noise\n",
    "            X.append(signal)\n",
    "            y.append(3)\n",
    "        \n",
    "        self.X = np.array(X, dtype=np.float32)\n",
    "        self.y = np.array(y, dtype=np.int64)\n",
    "        \n",
    "        # Shuffle\n",
    "        idx = np.arange(len(self.X))\n",
    "        np.random.shuffle(idx)\n",
    "        self.X = self.X[idx]\n",
    "        self.y = self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create dataset and data loaders\n",
    "print(\"Creating multi-modal dataset...\")\n",
    "dataset = MultiModalDataset(n_samples=10000, seq_len=32)\n",
    "\n",
    "# Split data\n",
    "n = len(dataset)\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)\n",
    "train_idx = indices[:int(n * 0.8)]\n",
    "val_idx = indices[int(n * 0.8):]\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx.tolist())\n",
    "val_dataset = Subset(dataset, val_idx.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Task types: 4 (Sinusoidal, Polynomial, Step Functions, Exponential)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardFFN(nn.Module):\n",
    "    \"\"\"Standard Feed-Forward Network for comparison baseline.\"\"\"\n",
    "    def __init__(self, d_model=32, d_hidden=160, n_classes=4):\n",
    "        super(StandardFFN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(d_hidden, d_hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden//2, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    \"\"\"Mixture of Experts model with specialized expert networks.\"\"\"\n",
    "    def __init__(self, d_model=32, d_hidden=80, n_experts=4, n_classes=4, top_k=2, load_balance_loss_coef=0.01):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        self.load_balance_loss_coef = load_balance_loss_coef\n",
    "        \n",
    "        # Router network with proper initialization for stability\n",
    "        self.router = nn.Sequential(\n",
    "            nn.Linear(d_model, d_hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.05),  # Reduced dropout for stability\n",
    "            nn.Linear(d_hidden//2, n_experts)\n",
    "        )\n",
    "        \n",
    "        # Initialize router weights for stability\n",
    "        with torch.no_grad():\n",
    "            for module in self.router.modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.normal_(module.weight, 0, 0.1)\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        \n",
    "        # Specialized experts with distinct architectures\n",
    "        self.experts = nn.ModuleList()\n",
    "        \n",
    "        # Expert 0: Sinusoidal specialist (Tanh for periodicity)\n",
    "        self.experts.append(nn.Sequential(\n",
    "            nn.Linear(d_model, d_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_hidden, d_hidden//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_hidden//2, n_classes)\n",
    "        ))\n",
    "        \n",
    "        # Expert 1: Polynomial specialist (Deep ReLU)\n",
    "        self.experts.append(nn.Sequential(\n",
    "            nn.Linear(d_model, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden, d_hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden//2, n_classes)\n",
    "        ))\n",
    "        \n",
    "        # Expert 2: Step function specialist\n",
    "        self.experts.append(nn.Sequential(\n",
    "            nn.Linear(d_model, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_hidden, d_hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden//2, n_classes)\n",
    "        ))\n",
    "        \n",
    "        # Expert 3: Exponential specialist (ELU for smooth gradients)\n",
    "        self.experts.append(nn.Sequential(\n",
    "            nn.Linear(d_model, d_hidden),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_hidden, d_hidden//2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(d_hidden//2, n_classes)\n",
    "        ))\n",
    "        \n",
    "        # Initialize expert weights properly\n",
    "        for expert in self.experts:\n",
    "            for module in expert.modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.expert_usage = torch.zeros(n_experts)\n",
    "        self.class_expert_matrix = torch.zeros(n_classes, n_experts)\n",
    "        \n",
    "    def forward(self, x, track_routing=False):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Router decision with temperature scaling for stability\n",
    "        gate_logits = self.router(x)\n",
    "        gate_probs = F.softmax(gate_logits / 1.0, dim=-1)  # Temperature=1.0\n",
    "        \n",
    "        # Top-k routing with proper normalization\n",
    "        topk_vals, topk_idx = torch.topk(gate_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Create routing weights\n",
    "        routing_weights = torch.zeros_like(gate_probs)\n",
    "        routing_weights.scatter_(1, topk_idx, topk_vals)\n",
    "        \n",
    "        # Normalize routing weights properly\n",
    "        routing_weights = routing_weights / (routing_weights.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Expert outputs\n",
    "        expert_outputs = []\n",
    "        for expert in self.experts:\n",
    "            expert_outputs.append(expert(x))\n",
    "        expert_stack = torch.stack(expert_outputs, dim=-1)  # [batch, n_classes, n_experts]\n",
    "        \n",
    "        # Apply routing weights\n",
    "        routing_weights_expanded = routing_weights.unsqueeze(1)  # [batch, 1, n_experts]\n",
    "        output = (expert_stack * routing_weights_expanded).sum(dim=-1)  # [batch, n_classes]\n",
    "        \n",
    "        # Calculate load balancing loss for training stability\n",
    "        self.load_balance_loss = 0.0\n",
    "        if self.training:\n",
    "            # Expert usage should be balanced\n",
    "            expert_usage = routing_weights.sum(dim=0)  # [n_experts]\n",
    "            balance_target = batch_size * self.top_k / self.n_experts\n",
    "            balance_loss = ((expert_usage - balance_target) ** 2).mean()\n",
    "            self.load_balance_loss = self.load_balance_loss_coef * balance_loss\n",
    "        \n",
    "        # Clean tracking for analysis\n",
    "        if track_routing:\n",
    "            with torch.no_grad():\n",
    "                self.expert_usage += gate_probs.sum(dim=0)\n",
    "            return output, {'gate_probs': gate_probs.detach(), 'routing_weights': routing_weights.detach()}\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def get_active_parameters(self):\n",
    "        \"\"\"Estimate active parameters based on expert usage.\"\"\"\n",
    "        if self.expert_usage.sum() == 0:\n",
    "            return self.count_parameters()\n",
    "        \n",
    "        # Router is always active\n",
    "        router_params = sum(p.numel() for p in self.router.parameters())\n",
    "        \n",
    "        # Weighted expert parameters\n",
    "        usage_weights = self.expert_usage / self.expert_usage.sum()\n",
    "        expert_params = 0\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_param_count = sum(p.numel() for p in expert.parameters())\n",
    "            expert_params += expert_param_count * usage_weights[i].item()\n",
    "        \n",
    "        return router_params + expert_params\n",
    "\n",
    "# Initialize models\n",
    "print(\"Initializing models...\")\n",
    "ffn = StandardFFN(d_model=32, d_hidden=160, n_classes=4).to(DEVICE)\n",
    "moe = MixtureOfExperts(d_model=32, d_hidden=80, n_experts=4, n_classes=4, top_k=2, load_balance_loss_coef=0.01).to(DEVICE)\n",
    "\n",
    "print(f\"FFN Parameters: {ffn.count_parameters():,}\")\n",
    "print(f\"MoE Parameters: {moe.count_parameters():,}\")\n",
    "print(f\"Parameter ratio (MoE/FFN): {moe.count_parameters()/ffn.count_parameters():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loader, criterion, track_routing=False):\n",
    "    \"\"\"Train model for one epoch with proper gradient clipping and loss tracking.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    routing_data = []\n",
    "    \n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if hasattr(model, 'expert_usage') and track_routing:\n",
    "            outputs, routing_info = model(batch_x, track_routing=True)\n",
    "            routing_data.append(routing_info)\n",
    "        else:\n",
    "            outputs = model(batch_x)\n",
    "        \n",
    "        # Main classification loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Add load balancing loss for MoE\n",
    "        if hasattr(model, 'load_balance_loss'):\n",
    "            loss = loss + model.load_balance_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability (especially important for MoE)\n",
    "        if hasattr(model, 'expert_usage'):  # MoE model\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        else:  # FFN model\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += predicted.eq(batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, routing_data\n",
    "\n",
    "def evaluate_model(model, loader, criterion, track_routing=False):\n",
    "    \"\"\"Evaluate model on validation data with routing analysis.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    class_expert_usage = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
    "            \n",
    "            if hasattr(model, 'expert_usage') and track_routing:\n",
    "                outputs, routing_info = model(batch_x, track_routing=True)\n",
    "                \n",
    "                # Track class-expert associations\n",
    "                gate_probs = routing_info['gate_probs']\n",
    "                for i, class_id in enumerate(batch_y.cpu().numpy()):\n",
    "                    for expert_id in range(gate_probs.size(1)):\n",
    "                        class_expert_usage[class_id][expert_id] += gate_probs[i, expert_id].item()\n",
    "            else:\n",
    "                outputs = model(batch_x)\n",
    "            \n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += predicted.eq(batch_y).sum().item()\n",
    "    \n",
    "    return total_loss / total, correct / total, dict(class_expert_usage)\n",
    "\n",
    "# Set up optimizers and learning rate schedulers\n",
    "optimizer_ffn = torch.optim.Adam(ffn.parameters(), lr=2e-3, weight_decay=1e-5, eps=1e-8)\n",
    "optimizer_moe = torch.optim.Adam(moe.parameters(), lr=1.5e-3, weight_decay=1e-5, eps=1e-8)  # Slightly lower LR for MoE\n",
    "\n",
    "scheduler_ffn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ffn, mode='min', factor=0.7, patience=10)\n",
    "scheduler_moe = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_moe, mode='min', factor=0.7, patience=10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training configuration ready!\")\n",
    "print(f\"FFN Learning Rate: {optimizer_ffn.param_groups[0]['lr']}\")\n",
    "print(f\"MoE Learning Rate: {optimizer_moe.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "epochs = 100\n",
    "print(f\"Starting training for {epochs} epochs...\")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'ffn_train_loss': [], 'ffn_train_acc': [], 'ffn_val_loss': [], 'ffn_val_acc': [],\n",
    "    'moe_train_loss': [], 'moe_train_acc': [], 'moe_val_loss': [], 'moe_val_acc': [],\n",
    "    'moe_active_params': [], 'moe_efficiency': [],\n",
    "    'ffn_total_params': ffn.count_parameters(),\n",
    "    'moe_total_params': moe.count_parameters()\n",
    "}\n",
    "\n",
    "best_moe_acc = 0\n",
    "final_class_expert_usage = {}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train FFN\n",
    "    ffn_train_loss, ffn_train_acc, _ = train_epoch(ffn, optimizer_ffn, train_loader, criterion)\n",
    "    ffn_val_loss, ffn_val_acc, _ = evaluate_model(ffn, val_loader, criterion)\n",
    "    \n",
    "    # Train MoE\n",
    "    track_routing = (epoch % 10 == 0) or (epoch == epochs)\n",
    "    moe_train_loss, moe_train_acc, _ = train_epoch(moe, optimizer_moe, train_loader, criterion, track_routing)\n",
    "    moe_val_loss, moe_val_acc, class_expert_usage = evaluate_model(moe, val_loader, criterion, track_routing)\n",
    "    \n",
    "    # Update learning rate schedulers\n",
    "    scheduler_ffn.step(ffn_val_loss)\n",
    "    scheduler_moe.step(moe_val_loss)\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    active_params = moe.get_active_parameters()\n",
    "    efficiency = active_params / ffn.count_parameters()\n",
    "    \n",
    "    # Store history\n",
    "    history['ffn_train_loss'].append(ffn_train_loss)\n",
    "    history['ffn_train_acc'].append(ffn_train_acc)\n",
    "    history['ffn_val_loss'].append(ffn_val_loss)\n",
    "    history['ffn_val_acc'].append(ffn_val_acc)\n",
    "    \n",
    "    history['moe_train_loss'].append(moe_train_loss)\n",
    "    history['moe_train_acc'].append(moe_train_acc)\n",
    "    history['moe_val_loss'].append(moe_val_loss)\n",
    "    history['moe_val_acc'].append(moe_val_acc)\n",
    "    \n",
    "    history['moe_active_params'].append(active_params)\n",
    "    history['moe_efficiency'].append(efficiency)\n",
    "    \n",
    "    if moe_val_acc > best_moe_acc:\n",
    "        best_moe_acc = moe_val_acc\n",
    "        final_class_expert_usage = class_expert_usage\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:3d} | FFN: {ffn_val_acc:.3f} | MoE: {moe_val_acc:.3f} | Efficiency: {efficiency:.3f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ffn_final_loss, ffn_final_acc, _ = evaluate_model(ffn, val_loader, criterion)\n",
    "moe_final_loss, moe_final_acc, _ = evaluate_model(moe, val_loader, criterion)\n",
    "\n",
    "improvement = ((moe_final_acc - ffn_final_acc) / ffn_final_acc) * 100\n",
    "param_reduction = (1 - moe.get_active_parameters() / ffn.count_parameters()) * 100\n",
    "\n",
    "print(f\"Standard FFN   | Accuracy: {ffn_final_acc:.4f} | Loss: {ffn_final_loss:.4f}\")\n",
    "print(f\"MoE Network    | Accuracy: {moe_final_acc:.4f} | Loss: {moe_final_loss:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"Performance Improvement: {improvement:.2f}%\")\n",
    "print(f\"Parameter Reduction: {param_reduction:.1f}%\")\n",
    "print(f\"Active Parameters: {moe.get_active_parameters():,.0f} / {moe.count_parameters():,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate efficiency statistics\n",
    "final_efficiency = history['moe_efficiency'][-1]\n",
    "avg_efficiency = np.mean(history['moe_efficiency'][10:])  # After stabilization\n",
    "print(f\"\\nEfficiency Analysis:\")\n",
    "print(f\"Final efficiency: {final_efficiency:.3f}\")\n",
    "print(f\"Average efficiency (after epoch 10): {avg_efficiency:.3f}\")\n",
    "print(f\"Efficiency stabilization: {'✓ Stable' if np.std(history['moe_efficiency'][10:]) < 0.01 else '✗ Unstable'}\")\n",
    "\n",
    "# Expert usage summary\n",
    "print(f\"\\nExpert Usage Summary:\")\n",
    "expert_usage_normalized = moe.expert_usage / moe.expert_usage.sum()\n",
    "for i, usage in enumerate(expert_usage_normalized):\n",
    "    print(f\"Expert {i}: {usage:.3f} ({usage*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up clean plotting style\n",
    "plt.rcParams.update({\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'legend.fontsize': 10,\n",
    "    'lines.linewidth': 2.5,\n",
    "    'figure.dpi': 100\n",
    "})\n",
    "\n",
    "# Create training curves visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Accuracy plot\n",
    "epochs_range = range(1, len(history['ffn_val_acc']) + 1)\n",
    "ax1.plot(epochs_range, history['ffn_val_acc'], label='Standard FFN', color='#2E86AB', linewidth=3)\n",
    "ax1.plot(epochs_range, history['moe_val_acc'], label='MoE Network', color='#A23B72', linewidth=3)\n",
    "ax1.set_ylabel('Validation Accuracy', fontweight='bold')\n",
    "ax1.set_title('Model Performance Comparison', fontweight='bold', pad=20)\n",
    "ax1.legend(frameon=True, shadow=True)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0.5, 1.0)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(epochs_range, history['ffn_val_loss'], label='Standard FFN', color='#2E86AB', linewidth=3)\n",
    "ax2.plot(epochs_range, history['moe_val_loss'], label='MoE Network', color='#A23B72', linewidth=3)\n",
    "ax2.set_ylabel('Validation Loss', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "ax2.set_title('Training Loss Progression', fontweight='bold', pad=20)\n",
    "ax2.legend(frameon=True, shadow=True)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Training summary statistics\n",
    "print(\"Training Curve Analysis:\")\n",
    "print(f\"Final FFN accuracy: {history['ffn_val_acc'][-1]:.4f}\")\n",
    "print(f\"Final MoE accuracy: {history['moe_val_acc'][-1]:.4f}\")\n",
    "print(f\"Best FFN accuracy: {max(history['ffn_val_acc']):.4f}\")\n",
    "print(f\"Best MoE accuracy: {max(history['moe_val_acc']):.4f}\")\n",
    "print(f\"FFN convergence (epochs to 95%): {next((i for i, acc in enumerate(history['ffn_val_acc']) if acc > 0.95), 'N/A')}\")\n",
    "print(f\"MoE convergence (epochs to 95%): {next((i for i, acc in enumerate(history['moe_val_acc']) if acc > 0.95), 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter efficiency visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Parameter count comparison\n",
    "ffn_params = history['ffn_total_params']\n",
    "moe_total_params = history['moe_total_params']\n",
    "moe_active_params = history['moe_active_params'][-1]  # Final active params\n",
    "\n",
    "models = ['Standard FFN', 'MoE Total', 'MoE Active']\n",
    "param_counts = [ffn_params, moe_total_params, moe_active_params]\n",
    "colors = ['#2E86AB', '#F18F01', '#A23B72']\n",
    "\n",
    "bars = ax1.bar(models, param_counts, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "ax1.set_ylabel('Parameter Count', fontweight='bold')\n",
    "ax1.set_title('Parameter Efficiency Analysis', fontweight='bold', pad=20)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, param_counts):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + max(param_counts)*0.01,\n",
    "            f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Efficiency over time\n",
    "epochs_range = range(1, len(history['moe_efficiency']) + 1)\n",
    "ax2.plot(epochs_range, history['moe_efficiency'], color='#A23B72', linewidth=3)\n",
    "ax2.set_ylabel('Active Parameters Ratio', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "ax2.set_title('Parameter Efficiency Over Training', fontweight='bold', pad=20)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Add annotation for stabilization\n",
    "stabilization_epoch = 10\n",
    "ax2.axvline(x=stabilization_epoch, color='red', linestyle='--', alpha=0.7)\n",
    "ax2.text(stabilization_epoch + 2, 0.8, 'Efficiency\\nStabilizes', \n",
    "         fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display efficiency metrics\n",
    "print(\"Parameter Efficiency Metrics:\")\n",
    "print(f\"FFN Total Parameters: {ffn_params:,}\")\n",
    "print(f\"MoE Total Parameters: {moe_total_params:,}\")\n",
    "print(f\"MoE Active Parameters: {moe_active_params:,.0f}\")\n",
    "print(f\"\")\n",
    "print(f\"MoE Total has {((ffn_params - moe_total_params) / ffn_params * 100):.1f}% fewer total parameters than FFN\")\n",
    "print(f\"MoE Active uses {((moe_active_params / ffn_params) * 100):.1f}% of FFN's parameter count actively\")\n",
    "print(f\"Parameter efficiency: {(1 - moe_active_params / ffn_params) * 100:.1f}% reduction\")\n",
    "\n",
    "# Efficiency timeline analysis\n",
    "early_efficiency = np.mean(history['moe_efficiency'][:5])\n",
    "late_efficiency = np.mean(history['moe_efficiency'][-10:])\n",
    "print(f\"\")\n",
    "print(f\"Efficiency Evolution:\")\n",
    "print(f\"Early training (epochs 1-5): {early_efficiency:.3f}\")\n",
    "print(f\"Late training (last 10 epochs): {late_efficiency:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_ui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
