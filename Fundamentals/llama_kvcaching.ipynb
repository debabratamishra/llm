{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd424c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, time, contextlib, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b721cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model and datatype for benchmarking\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "DTYPE    = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c7652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def timed_cuda(label: str):\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"{label:<25}: {(time.time() - t0)*1e3:8.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b650b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttentionKV(LlamaAttention):\n",
    "    \"\"\"Same attention, but keeps at most `MAX_CACHE` tokens in the returned cache.\"\"\"\n",
    "    MAX_CACHE = 2048\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_value=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        attn_output, attn_weights, present_kv = super().forward(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # crop the cache if it exists and is too large\n",
    "        if present_kv is not None and self.MAX_CACHE > 0:\n",
    "            if hasattr(present_kv, 'key_cache') and hasattr(present_kv, 'value_cache'):\n",
    "                # DynamicCache or StaticCache format\n",
    "                if present_kv.key_cache[0].size(2) > self.MAX_CACHE:\n",
    "                    present_kv.key_cache[0] = present_kv.key_cache[0][:, :, -self.MAX_CACHE:, :]\n",
    "                    present_kv.value_cache[0] = present_kv.value_cache[0][:, :, -self.MAX_CACHE:, :]\n",
    "            elif isinstance(present_kv, tuple) and len(present_kv) == 2:\n",
    "                k, v = present_kv\n",
    "                if hasattr(k, 'size') and k.size(2) > self.MAX_CACHE:\n",
    "                    k = k[:, :, -self.MAX_CACHE:, :]\n",
    "                    v = v[:, :, -self.MAX_CACHE:, :]\n",
    "                    present_kv = (k, v)\n",
    "            else:\n",
    "                if hasattr(present_kv, '__len__'):\n",
    "                    print(f\"Length: {len(present_kv)}\")\n",
    "                    if len(present_kv) > 0:\n",
    "                        print(f\"First element type: {type(present_kv[0])}\")\n",
    "\n",
    "        return attn_output, attn_weights, present_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401689a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID, torch_dtype=DTYPE, device_map=\"auto\"\n",
    "    ).eval()\n",
    "\n",
    "    for layer in model.model.layers:\n",
    "        layer.self_attn.__class__ = LlamaAttentionKV\n",
    "    print(\"Custom KV cache initialized\")\n",
    "\n",
    "    prompt = \"Why do cats purr?\"\n",
    "    def run(use_cache: bool):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.inference_mode():\n",
    "            return model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                use_cache=use_cache,\n",
    "                do_sample=False,     # deterministic\n",
    "                temperature=None, top_p=None,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "    with timed_cuda(\"No‑cache (baseline)\"):\n",
    "        _ = run(False)\n",
    "\n",
    "    with timed_cuda(\"With KV cache\"):\n",
    "        _ = run(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7327a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157170e22955407294082d1f2cadbaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom KV cache initialized\n",
      "No‑cache (baseline)      :  6471.98 ms\n",
      "With KV cache            :  5155.16 ms\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
