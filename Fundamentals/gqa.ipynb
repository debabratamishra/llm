{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b640135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class StandardAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        attn = torch.einsum('bthd,bshd->bhts', q, k) * self.scale\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = torch.einsum('bhts,bshd->bthd', attn, v).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class GQAAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, n_kv_heads):\n",
    "        super().__init__()\n",
    "        assert n_heads % n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, self.head_dim * n_kv_heads)\n",
    "        self.v_proj = nn.Linear(dim, self.head_dim * n_kv_heads)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, T, self.n_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, T, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # Repeat k and v to match the number of query heads\n",
    "        repeat_factor = self.n_heads // self.n_kv_heads\n",
    "        k = k.repeat_interleave(repeat_factor, dim=2)\n",
    "        v = v.repeat_interleave(repeat_factor, dim=2)\n",
    "\n",
    "        attn = torch.einsum('bthd,bshd->bhts', q, k) * self.scale\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = torch.einsum('bhts,bshd->bthd', attn, v).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "def benchmark_attention(attn_module, input_tensor, label):\n",
    "    torch.cuda.empty_cache()\n",
    "    start = time.time()\n",
    "    output = attn_module(input_tensor)\n",
    "    end = time.time()\n",
    "    mem = torch.cuda.memory_allocated() / 1e6 if torch.cuda.is_available() else 0\n",
    "    print(f\"{label}: Output shape: {output.shape}, Time: {end - start:.4f}s, CUDA Mem: {mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25b0940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Standard Multi-Head Attention ===\n",
      "Standard Attention: Output shape: torch.Size([2, 128, 512]), Time: 0.0038s, CUDA Mem: 0.00 MB\n",
      "\n",
      "=== Grouped Query Attention (GQA) ===\n",
      "GQA Attention: Output shape: torch.Size([2, 128, 512]), Time: 0.0029s, CUDA Mem: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.manual_seed(0)\n",
    "    B, T, C = 2, 128, 512   # batch size, sequence length, embedding dim\n",
    "    n_heads = 16\n",
    "    n_kv_heads = 4\n",
    "\n",
    "    x = torch.randn(B, T, C).to(device)\n",
    "\n",
    "    print(\"=== Standard Multi-Head Attention ===\")\n",
    "    std_attn = StandardAttention(C, n_heads).to(device)\n",
    "    benchmark_attention(std_attn, x, \"Standard Attention\")\n",
    "\n",
    "    print(\"\\n=== Grouped Query Attention (GQA) ===\")\n",
    "    gqa_attn = GQAAttention(C, n_heads, n_kv_heads).to(device)\n",
    "    benchmark_attention(gqa_attn, x, \"GQA Attention\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc8b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
