{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebadfc8",
   "metadata": {},
   "source": [
    "Distributed Finetuning of LLM using HF accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install Dependencies ---\n",
    "# !pip install -q transformers datasets accelerate bitsandbytes peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f9780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790c0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mac specific\n",
    "print(torch.backends.mps.is_available())\n",
    "torch.backends.mps.is_built()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91edc9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    device_map=None,  # We let accelerate handle device placement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70222559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debabratamishra/miniconda3/envs/py312/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afee1caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdacbc5beef94918bd85cd2c6a8de582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\", split=\"train[:1%]\")  # Tiny subset\n",
    "\n",
    "def format_qa(example):\n",
    "    prompt = f\"Question: {example['question']} Context: {example['context']} Answer:\"\n",
    "    answer = example['answers']['text'][0] if len(example['answers']['text']) > 0 else ''\n",
    "    full_text = prompt + \" \" + answer\n",
    "    tokenized = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    prompt_len = len(tokenizer(prompt, truncation=True, max_length=512)[\"input_ids\"])\n",
    "    labels[:prompt_len] = [-100] * prompt_len  # Mask prompt in loss\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(format_qa, remove_columns=dataset.column_names)\n",
    "tokenized_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "008167ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debabratamishra/miniconda3/envs/py312/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(tokenized_dataset, shuffle=True, batch_size=2)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc57c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 - Loss: 2.0729\n",
      "Step 10 - Loss: 0.6383\n",
      "Step 15 - Loss: 0.0666\n",
      "Step 20 - Loss: 0.0497\n",
      "Step 25 - Loss: 0.0249\n",
      "Step 30 - Loss: 0.0350\n",
      "Step 35 - Loss: 0.0385\n",
      "Step 40 - Loss: 0.0115\n",
      "Step 45 - Loss: 0.0286\n",
      "Step 50 - Loss: 0.0190\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "MAX_TRAIN_STEPS = 50\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in train_loader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        step += 1\n",
    "        if step % 5 == 0:\n",
    "            accelerator.print(f\"Step {step} - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        if step >= MAX_TRAIN_STEPS:\n",
    "            break\n",
    "    if step >= MAX_TRAIN_STEPS:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ed3e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "\n",
    "unwrapped_model.save_pretrained(\"./finetuned_tinyllama_qa\")\n",
    "tokenizer.save_pretrained(\"./finetuned_tinyllama_qa\")\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "840b99b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France? Context: France is a country in Europe. Answer: Paris\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Question: What is the capital of France? Context: France is a country in Europe. Answer:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = unwrapped_model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
