{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a basic understanding how to leverage llama index framework for building RAG pipeline on top of local hosted LLMs using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following to begin with\n",
    "# !pip install llama-index\n",
    "# !pip install chromadb\n",
    "# !pip install llama-index-embeddings-ollama\n",
    "# !pip install llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be using the model weights for both the base LLM and the embedding generation model from Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_ollama_environment():\n",
    "    \"\"\"Configure Ollama models for both LLM and embeddings\"\"\"\n",
    "    Settings.llm = Ollama(\n",
    "        model=\"llama3.2:1b\",\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        temperature=0.3,\n",
    "        request_timeout=600.0\n",
    "    )\n",
    "    \n",
    "    Settings.embed_model = OllamaEmbedding(\n",
    "        model_name=\"snowflake-arctic-embed2:latest\",\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        ollama_additional_kwargs={\"mirostat\": 0}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the document processing pipeline below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ollama_ingestion_pipeline(data_dir: str = \"data\"):\n",
    "    \"\"\"End-to-end document processing with Ollama embeddings\"\"\"\n",
    "    configure_ollama_environment()\n",
    "    \n",
    "    # Load and chunk documents\n",
    "    documents = SimpleDirectoryReader(\n",
    "        input_dir=data_dir,\n",
    "        required_exts=[\".pdf\", \".txt\"],\n",
    "        recursive=True\n",
    "    ).load_data()\n",
    "\n",
    "    # Initialize ChromaDB vector store\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    vector_store = ChromaVectorStore(\n",
    "        chroma_collection=chroma_client.get_or_create_collection(\"ollama_rag\")\n",
    "    )\n",
    "    \n",
    "    # Create vector index\n",
    "    return VectorStoreIndex.from_documents(\n",
    "        documents=documents,\n",
    "        storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
    "        show_progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a basic query engine for searching through the vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ollama_query_engine(index, similarity_top_k: int = 5):\n",
    "    return index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k,\n",
    "        vector_store_query_mode=\"hybrid\",\n",
    "        alpha=0.5,\n",
    "        response_mode=\"compact\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the pipeline with a sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_rag_system():\n",
    "    index = create_ollama_ingestion_pipeline()\n",
    "    query_engine = create_ollama_query_engine(index)\n",
    "    query = \"What is the main topic in this paper?\"\n",
    "    response = query_engine.query(query)\n",
    "    print(f\"Query: {query}\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47137a53bd41483b826391d1e7ac3f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b00465adf64d709b0c3d61aea47acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main topic in this paper?\n",
      "Response: The main topic of this paper is the development, deployment, and potential applications of Large Language Models (LLMs) in healthcare settings.\n"
     ]
    }
   ],
   "source": [
    "test_ollama_rag_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
